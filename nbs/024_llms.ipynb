{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM standard library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our diagram's ability to use generic functions, we can create a standard library of functions that are useful for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.core import get_git_root, load_env, checkLogs,  json_render,json_undeclared_vars,disk_cache\n",
    "import openai \n",
    "from pydantic import BaseModel, create_model\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "import json\n",
    "import re\n",
    "from parse import parse\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "import os\n",
    "from stringdale.core import semaphore_decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "load_env()\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import (\n",
    "    Callable, \n",
    "    Type, \n",
    "    Optional, \n",
    "    List, \n",
    "    Dict, \n",
    "    Any, \n",
    "    Union,\n",
    "    Literal,\n",
    "    get_type_hints\n",
    ")\n",
    "from pydantic import (\n",
    "    BaseModel, \n",
    "    Field, \n",
    "    create_model,\n",
    "    ConfigDict\n",
    ")\n",
    "import inspect\n",
    "import sys\n",
    "import sqlmodel\n",
    "from typing import Optional\n",
    "# TODO make chat outputschema also get typing (like bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import instructor\n",
    "from openai import OpenAI,AsyncOpenAI\n",
    "from anthropic import Anthropic, AsyncAnthropic\n",
    "from pydantic import BaseModel\n",
    "from singleton_decorator import singleton\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "    # complete_raw\n",
    "        # chooses what provider and calls a provider specific caller\n",
    "        # by model name with provider override\n",
    "\n",
    "        # complete_open_ai\n",
    "        # complete_anthropic \n",
    "        # ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "## OpenAI clients\n",
    "@singleton\n",
    "def tools_client():\n",
    "    return instructor.from_openai(AsyncOpenAI(),mode=instructor.Mode.TOOLS)\n",
    "@singleton\n",
    "def json_client():\n",
    "    return instructor.from_openai(AsyncOpenAI(),mode=instructor.Mode.JSON)\n",
    "@singleton\n",
    "def raw_client():\n",
    "    return AsyncOpenAI()\n",
    "\n",
    "## Anthropic clients\n",
    "@singleton\n",
    "def tools_anthropic_client():\n",
    "    return instructor.from_anthropic(\n",
    "        AsyncAnthropic(),\n",
    "        mode=instructor.Mode.ANTHROPIC_TOOLS\n",
    "    )\n",
    "\n",
    "@singleton\n",
    "def raw_anthropic_client():\n",
    "    return AsyncAnthropic()\n",
    "    \n",
    "# Patching the Anthropics client with the instructor for enhanced capabilities\n",
    "@singleton\n",
    "def json_anthropic_client():\n",
    "    anthropic_client = instructor.from_anthropic(\n",
    "    AsyncAnthropic(),\n",
    "    mode=instructor.Mode.ANTHROPIC_JSON\n",
    ")\n",
    "    return anthropic_client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO collapse mcp tools and raw modes\n",
    "\n",
    "@disk_cache.cache(ignore=['response_model'])\n",
    "async def complete_open_ai(model, messages, response_model=None, response_schema=None, mode='json', seed=42, **kwargs):\n",
    "    \"\"\"\n",
    "    OpenAI-specific completion handler.\n",
    "    Chooses between clients based on mode and runs the completion.\n",
    "    \"\"\"\n",
    "    if mode == 'json':\n",
    "        client = json_client()\n",
    "    elif mode == 'tools':\n",
    "        client = tools_client()\n",
    "    elif mode == 'raw':\n",
    "        client = raw_client()\n",
    "        # For raw mode, we use the standard OpenAI client API\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        # Return the raw response content and usage\n",
    "        return completion.choices[0].message.content, usage\n",
    "    # TODO: can it return several choices?\n",
    "    elif mode == 'mcp_tools':\n",
    "        if 'mcp_tools' not in kwargs or not kwargs['mcp_tools']:\n",
    "            raise ValueError(\"No MCP tools provided; 'mcp_tools' is required and cannot be empty in 'mcp_tools' mode.\")\n",
    "        mcp_tools = kwargs.pop('mcp_tools')\n",
    "        # Convert MCP tools to OpenAI format\n",
    "        openai_tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema\n",
    "            }\n",
    "        } for tool in mcp_tools]\n",
    "        \n",
    "        client = raw_client()\n",
    "        \n",
    "        # First LLM call to determine which tool to use\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=openai_tools,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Accumulate usage from first call\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        # Parse response into unified format\n",
    "        message = completion.choices[0].message\n",
    "        result = {\n",
    "            \"text\": message.content if message.content else None\n",
    "        }\n",
    "        \n",
    "        # Extract tool calls if any\n",
    "        if message.tool_calls:\n",
    "            result[\"tool_calls\"] = []\n",
    "            for tool_call in message.tool_calls:\n",
    "                result[\"tool_calls\"].append({\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"input\": json.loads(tool_call.function.arguments),\n",
    "                    \"id\": tool_call.id\n",
    "                })\n",
    "        \n",
    "        return result, usage\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    \n",
    "    # For json and tools modes (using instructor)\n",
    "    response, completion = await client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "    usage = {\n",
    "        \"input_tokens\": completion.usage.prompt_tokens,\n",
    "        \"output_tokens\": completion.usage.completion_tokens\n",
    "    }\n",
    "    return response.model_dump_json(), usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mcp_tools_to_openai_kwargs(mcp_tools):\n",
    "    \"\"\"\n",
    "    Convert MCP tools to OpenAI's tools parameter format.\n",
    "    \n",
    "    Args:\n",
    "        mcp_tools: List of MCP Tool objects (can be empty)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'tools' key containing OpenAI-formatted tools, or empty dict if no tools\n",
    "    \"\"\"\n",
    "    if not mcp_tools:\n",
    "        return {}\n",
    "    \n",
    "    openai_tools = [{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": tool.inputSchema\n",
    "        }\n",
    "    } for tool in mcp_tools]\n",
    "    \n",
    "    return {\"tools\": openai_tools}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "@disk_cache.cache(ignore=['response_model'])\n",
    "async def complete_open_ai(model, messages, response_model=None, response_schema=None, mode='json', seed=42, **kwargs):\n",
    "    \"\"\"\n",
    "    OpenAI-specific completion handler.\n",
    "    Chooses between clients based on mode and runs the completion.\n",
    "    \"\"\"\n",
    "    if mode == 'json':\n",
    "        client = json_client()\n",
    "        response, completion = await client.chat.completions.create_with_completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        return response.model_dump_json(), usage\n",
    "\n",
    "    elif mode == 'tools':\n",
    "        client = tools_client()\n",
    "        response, completion = await client.chat.completions.create_with_completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        return response.model_dump_json(), usage\n",
    "\n",
    "    elif mode == 'mcp_tools' or mode == 'raw':\n",
    "        # Unified path: handle both raw (no tools) and mcp_tools cases\n",
    "        client = raw_client()\n",
    "        \n",
    "        # Get MCP tools from kwargs (default to empty list for 'raw' mode)\n",
    "        mcp_tools = kwargs.pop('mcp_tools', [])\n",
    "        \n",
    "        # Convert MCP tools to OpenAI format\n",
    "        tools_kwargs = mcp_tools_to_openai_kwargs(mcp_tools)\n",
    "        \n",
    "        # Filter out unsupported parameters if needed\n",
    "        openai_kwargs = {k: v for k, v in kwargs.items() \n",
    "                        if k not in ['print_prompt']}\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            seed=seed,\n",
    "            **tools_kwargs,  # Includes tools if any\n",
    "            **openai_kwargs\n",
    "        )\n",
    "        \n",
    "        # Extract usage information\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        \n",
    "        # Parse response into unified format\n",
    "        message = completion.choices[0].message\n",
    "        result = {\n",
    "            \"text\": message.content if message.content else None\n",
    "        }\n",
    "        \n",
    "        # Extract tool calls if any\n",
    "        if message.tool_calls:\n",
    "            result[\"tool_calls\"] = []\n",
    "            for tool_call in message.tool_calls:\n",
    "                result[\"tool_calls\"].append({\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"input\": json.loads(tool_call.function.arguments),\n",
    "                    \"id\": tool_call.id\n",
    "                })\n",
    "        \n",
    "        # For backward compatibility with 'raw' mode: if no tools and no tool_calls, return just text string\n",
    "        if mode == 'raw' and not result.get(\"tool_calls\") and not mcp_tools:\n",
    "            return result[\"text\"], usage\n",
    "        \n",
    "        return result, usage\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_path = str(get_git_root()/\"stringdale/mcp_weather_server.py\")\n",
    "config = {\n",
    "  \"mcpServers\": {\n",
    "    \"weather\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [weather_path]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "mcp_client = Client(config)\n",
    "async with mcp_client:\n",
    "    mcp_tools = await mcp_client.list_tools()\n",
    "\n",
    "example_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Seattle? \"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'text': None, 'tool_calls': [{'name': 'get_forecast', 'input': {'latitude': 47.6062, 'longitude': -122.3321}, 'id': 'call_yKAozcjA70c3BuAP6qKcYOyH'}]}\n",
      "Usage: {'input_tokens': 135, 'output_tokens': 26}\n"
     ]
    }
   ],
   "source": [
    "# Call complete_anthropic with mode='mcp_tools' and pass mcp_tools\n",
    "result, usage = await complete_open_ai(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=example_messages,\n",
    "    mode=\"mcp_tools\",\n",
    "    mcp_tools=mcp_tools\n",
    ")\n",
    "print(\"Result:\", result)\n",
    "print(\"Usage:\", usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mcp_tools_to_anthropic_kwargs(mcp_tools):\n",
    "    \"\"\"\n",
    "    Convert MCP tools to Anthropic's tools parameter format.\n",
    "    \n",
    "    Args:\n",
    "        mcp_tools: List of MCP Tool objects (can be empty)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'tools' key containing Anthropic-formatted tools, or empty dict if no tools\n",
    "    \"\"\"\n",
    "    if not mcp_tools:\n",
    "        return {}\n",
    "    \n",
    "    anthropic_tools = [{\n",
    "        \"name\": tool.name,\n",
    "        \"description\": tool.description,\n",
    "        \"input_schema\": tool.inputSchema\n",
    "    } for tool in mcp_tools]\n",
    "    \n",
    "    return {\"tools\": anthropic_tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@disk_cache.cache(ignore=['response_model'])\n",
    "async def complete_anthropic(model, messages, response_model=None, response_schema=None, mode='json', seed=42, **kwargs):\n",
    "    \"\"\"\n",
    "    Anthropic-specific completion handler.\n",
    "    Chooses between clients based on mode and runs the completion.\n",
    "    \"\"\"\n",
    "    def _extract_usage(usage_obj):\n",
    "        \"\"\"Extract usage information from Anthropic usage object\"\"\"\n",
    "        return {\n",
    "            \"input_tokens\": getattr(usage_obj, \"input_tokens\", 0),\n",
    "            \"output_tokens\": getattr(usage_obj, \"output_tokens\", 0)\n",
    "        }\n",
    "    \n",
    "    max_tokens = kwargs.get('max_tokens', 1024)\n",
    "    # Filter out unsupported Anthropic parameters\n",
    "    # Anthropic doesn't support 'stop' or 'seed' parameters\n",
    "    anthropic_kwargs = {k: v for k, v in kwargs.items() \n",
    "                        if k not in ['stop', 'seed', 'print_prompt', 'mcp_tools']}\n",
    "    \n",
    "    if mode == 'json':\n",
    "        client = json_anthropic_client()\n",
    "        response, completion = await client.chat.completions.create_with_completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            max_tokens=max_tokens,\n",
    "            **anthropic_kwargs\n",
    "        )\n",
    "        return response.model_dump_json(), _extract_usage(completion.usage)\n",
    "\n",
    "    elif mode == 'mcp_tools' or mode == 'raw':\n",
    "        # Unified path: handle both raw (no tools) and mcp_tools cases\n",
    "        client = raw_anthropic_client()\n",
    "        \n",
    "        # Get MCP tools from kwargs (default to empty list for 'raw' mode)\n",
    "        mcp_tools = kwargs.pop('mcp_tools', [])\n",
    "        \n",
    "        # Convert MCP tools to Anthropic format\n",
    "        tools_kwargs = mcp_tools_to_anthropic_kwargs(mcp_tools)\n",
    "        \n",
    "        # Extract system message (first system message), rest as regular messages\n",
    "        system = None\n",
    "        stripped = []\n",
    "        for m in messages:\n",
    "            role = m.get(\"role\")\n",
    "            content = m.get(\"content\", \"\")\n",
    "            if role == \"system\" and system is None:\n",
    "                system = content\n",
    "            else:\n",
    "                stripped.append({\"role\": role, \"content\": content})\n",
    "        \n",
    "        # Call Anthropic API\n",
    "        response = await client.messages.create(\n",
    "            model=model,\n",
    "            system=system if system else \"\",  # Anthropic allows empty string for system\n",
    "            messages=stripped,\n",
    "            max_tokens=max_tokens,\n",
    "            **tools_kwargs,  # Includes tools if any\n",
    "            **anthropic_kwargs\n",
    "        )\n",
    "        \n",
    "        # Parse response content blocks\n",
    "        text_parts = []\n",
    "        tool_use_blocks = []\n",
    "        \n",
    "        for block in (response.content or []):\n",
    "            if getattr(block, \"type\", None) == \"text\":\n",
    "                text_parts.append(block.text)\n",
    "            elif getattr(block, \"type\", None) == \"tool_use\":\n",
    "                # Extract tool use information\n",
    "                tool_use_blocks.append({\n",
    "                    \"name\": block.name,\n",
    "                    \"input\": block.input,  # Already a dict, no need to parse JSON\n",
    "                    \"id\": getattr(block, \"id\", None)  # Include id if available\n",
    "                })\n",
    "        \n",
    "        # Extract usage information\n",
    "        usage = _extract_usage(response.usage)\n",
    "        \n",
    "        # Return consistent structure: always a dict with 'text' and optionally 'tool_calls'\n",
    "        result = {\n",
    "            \"text\": \"\\n\".join(text_parts) if text_parts else None\n",
    "        }\n",
    "        if tool_use_blocks:\n",
    "            result[\"tool_calls\"] = tool_use_blocks\n",
    "        \n",
    "        # For backward compatibility with 'raw' mode: if no tools and no tool_calls, return just text string\n",
    "        if mode == 'raw' and not tool_use_blocks and not mcp_tools:\n",
    "            return result[\"text\"], usage\n",
    "        \n",
    "        return result, usage\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing mcp tools mode for anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'text': 'Okay, let me check the weather forecast for Seattle:', 'tool_calls': [{'name': 'get_forecast', 'input': {'latitude': 47.6062, 'longitude': -122.3321}, 'id': 'toolu_019XLKX5aQS59Ck58unkCH5b'}]}\n",
      "Usage: {'input_tokens': 501, 'output_tokens': 90}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call complete_anthropic with mode='mcp_tools' and pass mcp_tools\n",
    "result, usage = await complete_anthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    messages=example_messages,\n",
    "    mode=\"mcp_tools\",\n",
    "    mcp_tools=mcp_tools\n",
    ")\n",
    "print(\"Result:\", result)\n",
    "print(\"Usage:\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Okay, let me check the weather forecast for Seattle:',\n",
       " 'tool_calls': [{'name': 'get_forecast',\n",
       "   'input': {'latitude': 47.6062, 'longitude': -122.3321},\n",
       "   'id': 'toolu_019XLKX5aQS59Ck58unkCH5b'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Used: get_forecast, Arguments: {'latitude': 47.6062, 'longitude': -122.3321}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tool_calls = result.get('tool_calls', [])\n",
    "if not tool_calls:\n",
    "    raise ValueError(\"No tool calls found in the result.\")\n",
    "tool_name = tool_calls[0]['name']\n",
    "tool_args = tool_calls[0]['input']  # Already a dict, no need to parse JSON\n",
    "print(f\"Tool Used: {tool_name}, Arguments: {tool_args}\")\n",
    "\n",
    "# Execute the tool called by the LLM\n",
    "async with mcp_client:\n",
    "    tool_response = await mcp_client.call_tool(tool_name, tool_args)\n",
    "    tool_response_text = tool_response.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This Afternoon:\n",
      "Temperature: 49°F\n",
      "Wind: 2 mph ESE\n",
      "Forecast: Rain likely after 2pm. Mostly cloudy. High near 49, with temperatures falling to around 47 in the afternoon. East southeast wind around 2 mph. Chance of precipitation is 60%. New rainfall amounts less than a tenth of an inch possible.\n",
      "\n",
      "---\n",
      "\n",
      "Tonight:\n",
      "Temperature: 44°F\n",
      "Wind: 2 to 7 mph E\n",
      "Forecast: Rain. Mostly cloudy. Low around 44, with temperatures rising to around 46 overnight. East wind 2 to 7 mph. Chance of precipitation is 90%. New rainfall amounts between a tenth and quarter of an inch possible.\n",
      "\n",
      "---\n",
      "\n",
      "Saturday:\n",
      "Temperature: 53°F\n",
      "Wind: 7 to 10 mph SSE\n",
      "Forecast: Rain likely. Mostly cloudy. High near 53, with temperatures falling to around 51 in the afternoon. South southeast wind 7 to 10 mph. Chance of precipitation is 70%. New rainfall amounts between a tenth and quarter of an inch possible.\n",
      "\n",
      "---\n",
      "\n",
      "Saturday Night:\n",
      "Temperature: 45°F\n",
      "Wind: 8 to 20 mph SE\n",
      "Forecast: Rain. Cloudy, with a low around 45. Southeast wind 8 to 20 mph, with gusts as high as 33 mph. Chance of precipitation is 90%. New rainfall amounts between a tenth and quarter of an inch possible.\n",
      "\n",
      "---\n",
      "\n",
      "Sunday:\n",
      "Temperature: 50°F\n",
      "Wind: 9 to 20 mph S\n",
      "Forecast: Rain. Cloudy, with a high near 50. South wind 9 to 20 mph, with gusts as high as 33 mph. Chance of precipitation is 90%. New rainfall amounts between a tenth and quarter of an inch possible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tool_response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we're setting up  everything we need to get tools and session for mcp.\n",
    "We're going to update chat to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_path = str(get_git_root()/\"stringdale/mcp_weather_server.py\")\n",
    "config = {\n",
    "  \"mcpServers\": {\n",
    "    \"weather\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [weather_path]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "mcp_client = Client(config)\n",
    "async with mcp_client:\n",
    "    mcp_tools = await mcp_client.list_tools()\n",
    "\n",
    "example_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Seattle? \"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def complete_raw(model, messages, response_model=None, response_schema=None, mode='json', seed=42, provider=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function is used to complete a chat completion with instructor without having basemodels as input or output.\n",
    "    used for disk caching of results.\n",
    "    \n",
    "    Chooses what provider and calls a provider specific caller by model name with provider override.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., 'gpt-4', 'claude-3-haiku-20240307')\n",
    "        messages: List of message dicts\n",
    "        response_model: Pydantic model for structured output\n",
    "        response_schema: Schema for response (computed if response_model is provided)\n",
    "        mode: Completion mode ('raw', 'json', 'tools')\n",
    "        seed: Random seed\n",
    "        provider: Optional provider override ('openai', 'anthropic'). If None, inferred from model name.\n",
    "        **kwargs: Additional arguments passed to the provider.\n",
    "                  Important provider-specific requirements:\n",
    "                  - Anthropic: max_tokens is REQUIRED (e.g., max_tokens=1024)\n",
    "    \"\"\"\n",
    "    # Determine provider from model name or use override\n",
    "    if provider is None:\n",
    "        if model.startswith(('gpt-', 'o1-')):\n",
    "            provider = 'openai'\n",
    "        elif model.startswith('claude-'):\n",
    "            provider = 'anthropic'\n",
    "        else:\n",
    "            # Default to OpenAI for unknown models\n",
    "            provider = 'openai'\n",
    "    \n",
    "    # Route to provider-specific function\n",
    "    if provider == 'openai':\n",
    "        return await complete_open_ai(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            response_schema=response_schema,\n",
    "            mode=mode,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "    elif provider == 'anthropic':\n",
    "        return await complete_anthropic(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            response_schema=response_schema,\n",
    "            mode=mode,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "async def complete(model, messages, response_model, mode='json', print_prompt=False, **kwargs):\n",
    "    # Compute schema if response model provided\n",
    "    if isinstance(response_model, type) and issubclass(response_model, BaseModel):\n",
    "        response_schema = response_model.model_json_schema()\n",
    "    else:\n",
    "        response_schema = str(response_model)\n",
    "    if print_prompt:\n",
    "        print(messages)\n",
    "    response, usage = await complete_raw(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "        response_schema=response_schema,\n",
    "        mode=mode,\n",
    "        **kwargs\n",
    "    )\n",
    "    if mode == 'raw' or mode == 'mcp_tools':\n",
    "        return response, usage\n",
    "    else:\n",
    "        # Check if response is already a model instance (from instructor)\n",
    "        if isinstance(response, BaseModel):\n",
    "            return response, usage\n",
    "        else:\n",
    "            return response_model.model_validate_json(response), usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@disk_cache.cache(ignore=['response_model'])\n",
    "async def old_complete_raw(model, messages, response_model=None, response_schema=None, mode = 'json' , seed=42,**kwargs):\n",
    "    \"\"\"\n",
    "    This function is used to complete a chat completion with instructor without having basemodels as input or output.\n",
    "    used for disk caching of results.\n",
    "    \"\"\"\n",
    "    if mode == 'json':\n",
    "        client = json_client()\n",
    "    elif mode == 'tools':\n",
    "        client = tools_client()\n",
    "    elif mode == 'raw':\n",
    "        client = raw_client()\n",
    "        # For raw mode, we use the standard OpenAI client API\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            seed=seed,\n",
    "            **kwargs\n",
    "        )\n",
    "        usage = {\n",
    "            \"input_tokens\": completion.usage.prompt_tokens,\n",
    "            \"output_tokens\": completion.usage.completion_tokens\n",
    "        }\n",
    "        # Return the raw response content and usage\n",
    "        return completion.choices[0].message.content, usage\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    \n",
    "    response, completion = await client.chat.completions.create_with_completion(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "        seed=seed,\n",
    "        **kwargs\n",
    "    )\n",
    "    usage = {\n",
    "        \"input_tokens\": completion.usage.prompt_tokens,\n",
    "        \"output_tokens\": completion.usage.completion_tokens\n",
    "    }\n",
    "    return response.model_dump_json(), usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def old_complete(model, messages, response_model,mode='json',print_prompt=False,**kwargs):\n",
    "    # Compute schema if response model provided\n",
    "    if isinstance(response_model, type) and issubclass(response_model, BaseModel):\n",
    "        response_schema = response_model.model_json_schema()\n",
    "    else:\n",
    "        response_schema = str(response_model)\n",
    "    if print_prompt:\n",
    "        print(messages)\n",
    "    response, usage = await complete_raw(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=response_model,\n",
    "        response_schema=response_schema,\n",
    "        mode=mode,\n",
    "        **kwargs\n",
    "    )\n",
    "    if mode == 'raw':\n",
    "        return response, usage\n",
    "    else:\n",
    "        return response_model.model_validate_json(response), usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(UserExtract(name='Extract jason', age=25),\n",
       " {'input_tokens': 147, 'output_tokens': 18})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class UserExtract(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "user, usage = await complete(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserExtract,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "user,usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(UserExtract(name='jason', age=25),\n",
       " {'input_tokens': 326, 'output_tokens': 1110})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user, usage = await complete(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    response_model=UserExtract,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "user,usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def answer_question(model,messages,**api_kwargs):\n",
    "    res,usage = await complete(model,messages,response_model=None,mode='raw',**api_kwargs)\n",
    "    return res,usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The capital of France is Paris.', {'input_tokens': 14, 'output_tokens': 7})\n"
     ]
    }
   ],
   "source": [
    "answer = await answer_question(\"gpt-3.5-turbo\",[{\"role\":\"user\",\"content\":\"What is the capital of France?\"}])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Integration test with answer_question - anthropic\n",
    "answer, usage = await answer_question(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    ")\n",
    "\n",
    "assert isinstance(answer, str)\n",
    "assert \"Paris\" in answer or \"paris\" in answer.lower()\n",
    "assert isinstance(usage, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def choose(model,messages,choices,**api_kwargs):\n",
    "    class Choice(BaseModel):\n",
    "        choice: Literal[tuple(choices)]\n",
    "    res,usage = await complete(model,messages,Choice,**api_kwargs)\n",
    "    return res.choice,usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is the capital of the country France?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('PARIS', {'input_tokens': 140, 'output_tokens': 10})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await choose(\"gpt-3.5-turbo\",[{\"role\":\"user\",\"content\":\"What is the capital of the country France?\"}],[\"PARIS\", \"HILTON\"],print_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is the capital of the country France?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('PARIS', {'input_tokens': 154, 'output_tokens': 92})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await choose(\"claude-sonnet-4-5\",[{\"role\":\"user\",\"content\":\"What is the capital of the country France?\"}],[\"PARIS\", \"HILTON\"],print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def choose_many(model,messages,choices,**api_kwargs):\n",
    "    class Choice(BaseModel):\n",
    "        choice: Literal[tuple(choices)]\n",
    "\n",
    "    class Choices(BaseModel):\n",
    "        choices: List[Choice]   \n",
    "    res,usage = await complete(model,messages,Choices,**api_kwargs)\n",
    "    return [c.choice for c in res.choices],usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'what parameters did i pass in? my name is jason and i am 25 years old'}]\n",
      "(['Name', 'Age'], {'input_tokens': 235, 'output_tokens': 31})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_answer = await choose_many(\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"what parameters did i pass in? my name is jason and i am 25 years old\"}],\n",
    "    choices=[\"Age\", \"Name\",\"City\"],print_prompt=True)\n",
    "\n",
    "print(new_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'what parameters did i pass in? my name is jason and i am 25 years old'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Name', 'Age'], {'input_tokens': 272, 'output_tokens': 105})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calude\n",
    "await choose_many(\"claude-sonnet-4-5\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"what parameters did i pass in? my name is jason and i am 25 years old\"}],\n",
    "    choices=[\"Age\", \"Name\",\"City\"],print_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_model(model: Type[sqlmodel.SQLModel], name: Optional[str] = None) -> Type[BaseModel]:\n",
    "    \"\"\"Convert an SQLModel to a Pydantic BaseModel.\n",
    "    used to clean up the output for the LLM\n",
    "    Args:\n",
    "        model: SQLModel class to convert\n",
    "        name: Optional name for the new model class\n",
    "        \n",
    "    Returns:\n",
    "        A Pydantic BaseModel class with the same fields\n",
    "    \"\"\"\n",
    "    # Get field definitions from the SQLModel\n",
    "    fields = {}\n",
    "    for field_name, field in model.model_fields.items():\n",
    "        fields[field_name] = (field.annotation, field)\n",
    "    \n",
    "    # Create new model name if not provided\n",
    "    model_name = name or f\"{model.__name__}Schema\"\n",
    "    \n",
    "    # Create and return new Pydantic model\n",
    "    return create_model(model_name, **fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def structured_output(model,messages,output_schema,as_json=False,**api_kwargs):\n",
    "\n",
    "    is_sqlmodel = isinstance(output_schema,type) and issubclass(output_schema,sqlmodel.SQLModel)\n",
    "    if is_sqlmodel:\n",
    "        clean_schema = clean_model(output_schema)\n",
    "    else:\n",
    "        clean_schema = output_schema\n",
    "\n",
    "    res,usage = await complete(model,messages,clean_schema,**api_kwargs)\n",
    "\n",
    "    if is_sqlmodel:\n",
    "        res = output_schema(**res.model_dump())\n",
    "    if as_json:\n",
    "        res = res.model_dump()\n",
    "    return res,usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Jason' age=25\n",
      "{'input_tokens': 157, 'output_tokens': 16}\n"
     ]
    }
   ],
   "source": [
    "class UserExtract(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "\n",
    "res,usage = await structured_output(\"gpt-3.5-turbo\",\n",
    "    [{\"role\":\"user\",\"content\":\"what parameters did i pass in? my name is Jason and i am 25 years old\"}],\n",
    "    UserExtract)\n",
    "assert res == UserExtract(name=\"Jason\",age=25), res\n",
    "print(res)\n",
    "print(usage)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Jason' age=25\n",
      "{'input_tokens': 174, 'output_tokens': 132}\n"
     ]
    }
   ],
   "source": [
    "# claude\n",
    "res,usage = await structured_output(\"claude-sonnet-4-5\",\n",
    "    [{\"role\":\"user\",\"content\":\"what parameters did i pass in? my name is Jason and i am 25 years old\"}],\n",
    "    UserExtract)\n",
    "\n",
    "print(res)\n",
    "print(usage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "from sqlalchemy.engine import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "class User(sqlmodel.SQLModel, table=False):\n",
    "    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n",
    "    name: Optional[str] = sqlmodel.Field(default=None)\n",
    "    age: Optional[int] = sqlmodel.Field(default=None)\n",
    "    email: Optional[str] = sqlmodel.Field(default=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='jyson' age=25 email=None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res,usage = await structured_output(\"gpt-3.5-turbo\",\n",
    "    [{\"role\":\"user\",\"content\":\"my name is jyson, my age is 25, my id is 1\"}],\n",
    "    User)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='jyson' age=25 email=None\n"
     ]
    }
   ],
   "source": [
    "# anthropic\n",
    "anth_res,usage = await structured_output(\"claude-sonnet-4-5\",\n",
    "    [{\"role\":\"user\",\"content\":\"my name is jyson, my age is 25, my id is 1\"}],\n",
    "    User)\n",
    "print(res)\n",
    "assert anth_res==res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import docstring_parser \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def function_to_input_description(func: Callable) -> Dict[str, Any]:\n",
    "    \"\"\"Extract parameter information from a function's signature and docstring.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - name: Function name\n",
    "            - params: Dict of parameter info with:\n",
    "                - type: Parameter type annotation\n",
    "                - description: Parameter description from docstring\n",
    "                - default: Default value if any\n",
    "    \"\"\"\n",
    "    # Get type hints and signature\n",
    "    hints = get_type_hints(func)\n",
    "    sig = inspect.signature(func)\n",
    "    \n",
    "    # Remove return annotation if present\n",
    "    hints.pop('return', None)\n",
    "    \n",
    "    # Parse docstring for parameter descriptions\n",
    "    docstring = inspect.getdoc(func)\n",
    "    arg_descriptions = {}\n",
    "    if docstring:\n",
    "        parsed = docstring_parser.parse(docstring)\n",
    "        arg_descriptions = {\n",
    "            p.arg_name: p.description \n",
    "            for p in parsed.params\n",
    "        }\n",
    "    \n",
    "    # Build parameter info\n",
    "    params = {}\n",
    "    for name, param in sig.parameters.items():\n",
    "        params[name] = {\n",
    "            'type': hints.get(name, Any),\n",
    "            'description': arg_descriptions.get(name, f\"Parameter {name}\"),\n",
    "            'default': None if param.default == inspect.Parameter.empty else param.default\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'name': func.__name__,\n",
    "        'params': params\n",
    "    }\n",
    "\n",
    "def description_to_model(desc: Dict[str, Any], model_name: Optional[str] = None) -> Type[BaseModel]:\n",
    "    \"\"\"Create a Pydantic model from a function description.\n",
    "    \n",
    "    Args:\n",
    "        desc: Function description from function_to_input_description\n",
    "        model_name: Optional name for the model class\n",
    "        \n",
    "    Returns:\n",
    "        Pydantic model class\n",
    "    \"\"\"\n",
    "    # Create model fields\n",
    "    fields = {}\n",
    "    \n",
    "    for name, info in desc['params'].items():\n",
    "        field = Field(\n",
    "            description=info['description'],\n",
    "            default=info['default'] if info['default'] is not None else ...\n",
    "        )\n",
    "        fields[name] = (info['type'], field)\n",
    "    \n",
    "    # Create model class\n",
    "    model_name = model_name or f\"{desc['name']}Input\"\n",
    "    return create_model(\n",
    "        model_name, \n",
    "        __config__=ConfigDict(extra='forbid'),\n",
    "        **fields\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add(a: int, b: int = 0) -> int:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First number to add\n",
    "        b: Second number to add\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Get function description\n",
    "desc = function_to_input_description(add)\n",
    "assert desc == {\n",
    "    'name': 'add',\n",
    "    'params': {'a': {'type': int,'description': 'First number to add','default': None},\n",
    "               'b': {'type': int, 'description': 'Second number to add', 'default': 0}\n",
    "               }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_op(op: Literal[\"add\", \"multiply\", \"divide\"],a: int=0, b: int = 0) -> int:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        op: Operation to perform\n",
    "        a: First number to add\n",
    "        b: Second number to add\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Get function description\n",
    "desc = function_to_input_description(math_op)\n",
    "desc\n",
    "assert desc == {'name': 'math_op',\n",
    " 'params': {'op': {'type': Literal['add', 'multiply', 'divide'],\n",
    "   'description': 'Operation to perform',\n",
    "   'default': None},\n",
    "  'a': {'type': int, 'description': 'First number to add', 'default': 0},\n",
    "  'b': {'type': int, 'description': 'Second number to add', 'default': 0}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def function_to_input_model(func: Callable,name:str,descriminator_field:str=\"tool_name\") -> Type[BaseModel]:\n",
    "    \"\"\"Convert a function to a Pydantic input model.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Pydantic model class for function inputs\n",
    "    \"\"\"\n",
    "    desc = function_to_input_description(func)\n",
    "    desc['params'][descriminator_field] = {\n",
    "        'type': Literal[name],\n",
    "        'description': 'The name of the function to call',\n",
    "        'default': None\n",
    "    }\n",
    "    return description_to_model(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def call_tools(\n",
    "    model: str,\n",
    "    messages: List[Dict[str, str]], \n",
    "    tools: Dict[str, Callable],\n",
    "    call_function: bool=False,\n",
    "    descriminator_field:str=\"tool_name\",\n",
    "    **api_kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Call OpenAI chat completion with tool selection and input parsing.\n",
    "    \n",
    "    Args:\n",
    "        model: OpenAI model name\n",
    "        messages: List of message dicts with role and content\n",
    "        tools: Dictionary mapping tool names to functions\n",
    "        descriminator_field: The name of the field to use as the discriminator\n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - tool_name: Selected tool name\n",
    "            - tool_input: Parsed input for the tool\n",
    "    \"\"\"\n",
    "    # Create input models for each tool\n",
    "    tool_models = {\n",
    "        name: function_to_input_model(func,name,descriminator_field)\n",
    "        for name, func in tools.items()\n",
    "    }\n",
    "    \n",
    "    # Create discriminated union model for tool inputs\n",
    "    class ToolsInput(BaseModel):\n",
    "        tool_input: Union[tuple(tool_models.values())] = Field(discriminator=descriminator_field)\n",
    "    \n",
    "    tool_description = '\\n'.join([f'{name}:{inspect.getdoc(func)}' for name,func in tools.items()])\n",
    "    system_message = [\n",
    "        {'role':'system',\n",
    "        'content':f'choose an appropriate tool to use to answer the following thought based on the following tools:\\n'\n",
    "                  f'{tool_description}'\n",
    "        }\n",
    "    ]\n",
    "    messages = system_message + messages\n",
    "\n",
    "    # Get tool input from LLM\n",
    "    response, usage = await complete(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_model=ToolsInput,\n",
    "        mode='tools',\n",
    "        **api_kwargs\n",
    "    )\n",
    "    \n",
    "    kwargs = response.tool_input.model_dump()\n",
    "    func_name = kwargs.pop(descriminator_field)\n",
    "\n",
    "    result = {\n",
    "        'name': func_name,\n",
    "        'input': kwargs,\n",
    "    }\n",
    "    if call_function:\n",
    "        try:\n",
    "            result['output'] = tools[func_name](**kwargs)\n",
    "        except Exception as e:\n",
    "            result['output'] = f'Error: {str(e)}'\n",
    "    return result,usage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int = 0) -> int:\n",
    "    \"\"\"Add two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First number to add\n",
    "        b: Second number to add\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\n",
    "    \n",
    "    Args:\n",
    "        x: First number to multiply\n",
    "        y: Second number to multiply\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "def divide(numerator: int, denominator: int) -> float:\n",
    "    \"\"\"Divide two numbers.\n",
    "    \n",
    "    Args:\n",
    "        numerator: Number to divide\n",
    "        denominator: Number to divide by\n",
    "    \"\"\"\n",
    "    return numerator / denominator\n",
    "\n",
    "tools = {\n",
    "    'add': add,\n",
    "    'multiply': multiply,\n",
    "    'divide': divide\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'choose an appropriate tool to use to answer the following thought based on the following tools:\\nadd:Add two numbers.\\n\\nArgs:\\n    a: First number to add\\n    b: Second number to add\\nmultiply:Multiply two numbers.\\n\\nArgs:\\n    x: First number to multiply\\n    y: Second number to multiply\\ndivide:Divide two numbers.\\n\\nArgs:\\n    numerator: Number to divide\\n    denominator: Number to divide by'}, {'role': 'user', 'content': 'What is 5 plus 3?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'name': 'add', 'input': {'a': 5, 'b': 3}},\n",
       " {'input_tokens': 335, 'output_tokens': 17})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result,usage = await call_tools(\n",
    "    model=\"gpt-3.5-turbo\",  \n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 5 plus 3?\"}],\n",
    "    tools=tools,\n",
    "    print_prompt=True\n",
    ")\n",
    "assert result == {'name': 'add', 'input': {'a': 5, 'b': 3}}\n",
    "result,usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main chat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy,copy\n",
    "from pprint import pformat,pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Chat:\n",
    "    \"\"\"A Chat objects the renders a prompt and calls an LLM. Currently supporting openai models.\n",
    "    \n",
    "    Args:\n",
    "        model: OpenAI model name\n",
    "        messages: List of message dicts, must have at least a role and content field\n",
    "        output_schema: Optional schema for structured output\n",
    "        as_json: Optional boolean to return the response as a json object\n",
    "        tools: Optional dictionary of tool names and functions that the LLM can decide to call. Causes the content of the response\n",
    "            to be a dict of the form {'name':tool_name,'input':tool_input_dict}\n",
    "        call_function: if tools are provided, whether to call the function and save the output in the output field of the response's content\n",
    "        choices: Optional List of choices for multi-choice questions\n",
    "        multi_choice: if choices are provided, whether to choose multiple items from the list\n",
    "        seed: Optional seed for random number generation\n",
    "        stop: Optional string or list of strings where the model should stop generating\n",
    "        save_history: Optional boolean to save the history of the chat between calls\n",
    "        append_output: Optional, whether to append the output of the chat to history automatically, default False\n",
    "        init_messages: Optional list of messages that are always prepended to messages.\n",
    "            Useful for supplying additional messages during calls.\n",
    "            Can have template variables that are fed during initialization only.\n",
    "            If save_history is True, the init messages are added to the history.\n",
    "        **kwargs: Keyword arguments to interpolate into the messages\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        model: Optional[str] = None,\n",
    "        messages: Optional[List[Dict[str, str]]] = None, \n",
    "        output_schema: Optional[BaseModel] = None,\n",
    "        as_json: Optional[bool] = False,\n",
    "        tools: Optional[Dict[str,Callable]] = None,\n",
    "        mcp_tools: Optional[Dict[str,Callable]] = None,\n",
    "        call_function: Optional[bool] = False,\n",
    "        choices: Optional[Enum] = None,\n",
    "        multi_choice: Optional[bool] = False,\n",
    "        seed: Optional[int] = 42,\n",
    "        stop: Optional[Union[str, List[str]]] = None,\n",
    "        log_prompt: bool = False,\n",
    "        save_history: bool = False,\n",
    "        append_output: bool = False,\n",
    "        init_messages: Optional[List[Dict[str, str]]] = None,\n",
    "        **kwargs):\n",
    "\n",
    "        self.model = model\n",
    "        self.messages = deepcopy(messages)\n",
    "        self.output_schema = output_schema\n",
    "        self.as_json = as_json\n",
    "        self.tools = tools\n",
    "        self.mcp_tools = mcp_tools\n",
    "        self.call_function = call_function\n",
    "        self.choices = choices\n",
    "        self.multi_choice = multi_choice\n",
    "        self.seed = seed\n",
    "        self.stop = stop\n",
    "        self.log_prompt = log_prompt\n",
    "        self.baked_kwargs = kwargs\n",
    "        self.save_history = save_history\n",
    "        self.append_output = append_output\n",
    "        \n",
    "        if init_messages is None:\n",
    "            init_messages = []\n",
    "        self.init_messages = json_render(init_messages,context=kwargs)\n",
    "    \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets state of Chat\"\"\"\n",
    "        if self.save_history:\n",
    "            self.history = []\n",
    "            self.history.extend(self.init_messages)\n",
    "        else:\n",
    "            self.history = None\n",
    "    \n",
    "    def dump_state(self):\n",
    "        \"\"\"dumps the node state\"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def load_state(self,state_object):\n",
    "        \"\"\"loads node state\"\"\"\n",
    "        self.history = state_object\n",
    "\n",
    "    def __copy__(self):\n",
    "        chat_copy = Chat(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            output_schema=self.output_schema,\n",
    "            as_json=self.as_json,\n",
    "            tools=self.tools,\n",
    "            mcp_tools=self.mcp_tools,\n",
    "            call_function=self.call_function,\n",
    "            choices=self.choices,\n",
    "            multi_choice=self.multi_choice,\n",
    "            seed=self.seed,\n",
    "            stop=self.stop,\n",
    "            log_prompt=self.log_prompt,\n",
    "            save_history=self.save_history,\n",
    "            append_output=self.append_output,\n",
    "            init_messages=self.init_messages,\n",
    "            **self.baked_kwargs\n",
    "        )\n",
    "        return chat_copy\n",
    "\n",
    "    async def __call__(self, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Format prompt with kwargs and call OpenAI chat.\n",
    "        Init parameters such as output_schema, tools, choices, seed, stop, as well as template variables\n",
    "        can be set or overridden by kwargs\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Values for format string placeholders\n",
    "            \n",
    "        Returns:\n",
    "            a dictionary with the following keys:\n",
    "            - role (str): Always \"assistant\"\n",
    "            - content: the llm response.\n",
    "            - meta (dict): Usage statistics including input and output tokens\n",
    "        \"\"\"\n",
    "        model = kwargs.get(\"model\", self.model)\n",
    "        messages = kwargs.get(\"messages\", self.messages)\n",
    "        output_schema = kwargs.get(\"output_schema\", self.output_schema)\n",
    "        as_json = kwargs.get(\"as_json\", self.as_json)\n",
    "        tools = kwargs.get(\"tools\", self.tools)\n",
    "        mcp_tools = kwargs.get(\"mcp_tools\", self.mcp_tools)\n",
    "        call_function = kwargs.get(\"call_function\", self.call_function)\n",
    "        choices = kwargs.get(\"choices\", self.choices)\n",
    "        multi_choice = kwargs.get(\"multi_choice\", self.multi_choice)\n",
    "        seed = kwargs.get(\"seed\", self.seed)\n",
    "        stop = kwargs.get(\"stop\", self.stop)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"model is required but not provided\")\n",
    "        if messages is None:\n",
    "            raise ValueError(\"messages is required but not provided\")\n",
    "\n",
    "        prompt_kwargs = {**self.baked_kwargs, **kwargs}\n",
    "\n",
    "        required_kwargs = json_undeclared_vars(messages)\n",
    "        if not required_kwargs <= set(prompt_kwargs):\n",
    "            missing = required_kwargs - set(prompt_kwargs)\n",
    "            raise ValueError(f\"Missing required kwargs: {missing}\")\n",
    "\n",
    "        formatted_messages = json_render(messages, context=prompt_kwargs)\n",
    "\n",
    "        if self.save_history:\n",
    "            self.history.extend(formatted_messages)\n",
    "            formatted_messages = self.history\n",
    "        else:\n",
    "            formatted_messages = self.init_messages + formatted_messages\n",
    "\n",
    "        if self.log_prompt:\n",
    "            logger.warning(f'calling llm with model={model} and prompt:\\n'\n",
    "                        f'messages={pformat(formatted_messages)}\\n'\n",
    "                        )\n",
    "\n",
    "        completion_kwargs = {\n",
    "            'model':model,\n",
    "            'messages':formatted_messages,\n",
    "            'seed':seed,\n",
    "            'stop':stop,\n",
    "            'print_prompt':prompt_kwargs.get('print_prompt',False),\n",
    "        }\n",
    "\n",
    "        if choices:\n",
    "            if multi_choice:\n",
    "                res,usage = await choose_many(choices=choices,**completion_kwargs)\n",
    "            else:\n",
    "                res,usage = await choose(choices=choices,**completion_kwargs)\n",
    "        elif output_schema:\n",
    "            res,usage = await structured_output(output_schema=output_schema,as_json=as_json,**completion_kwargs)\n",
    "        elif mcp_tools:\n",
    "            res, usage = await complete(\n",
    "                mode='mcp_tools',\n",
    "                mcp_tools=mcp_tools,\n",
    "                response_model=None,\n",
    "                **completion_kwargs\n",
    "            )\n",
    "        elif tools:\n",
    "            res,usage = await call_tools(tools=tools,call_function=call_function,**completion_kwargs)\n",
    "        else:\n",
    "            res,usage = await answer_question(**completion_kwargs)\n",
    "\n",
    "        response = {\n",
    "            'role':'assistant',\n",
    "            'content':res,\n",
    "            'meta':usage\n",
    "        }\n",
    "        if self.save_history and self.append_output:\n",
    "            self.history.append(response)\n",
    "        return response\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"String representation showing required keys, model, and output schema.\"\"\"\n",
    "        parts = [f\"Chat(model='{self.model}'\"]\n",
    "        \n",
    "        if self.messages:\n",
    "            required_keys = json_undeclared_vars(self.messages) - set(self.baked_kwargs.keys())\n",
    "            parts.append(f\"required_keys={required_keys}\")\n",
    "            \n",
    "        if self.output_schema:\n",
    "            parts.append(f\"output_schema={self.output_schema.__name__}\")\n",
    "        \n",
    "        if self.tools:\n",
    "            parts.append(f\"\"\"tools={\",\".join(self.tools.keys())}\"\"\")\n",
    "        if self.call_function:\n",
    "            parts.append(f\"call_function={self.call_function}\")\n",
    "            \n",
    "        # if self.choices:\n",
    "        #     parts.append(f\"choices={self.choices}\")\n",
    "        # if self.multi_choice:\n",
    "        #     parts.append(f\"multi_choice={self.multi_choice}\")\n",
    "            \n",
    "        if self.seed:\n",
    "            parts.append(f\"seed={self.seed}\")\n",
    "            \n",
    "        if self.stop:\n",
    "            parts.append(f\"stop={self.stop}\")\n",
    "\n",
    "        if self.save_history:\n",
    "            parts.append(f\"save_history={self.save_history}\")\n",
    "            \n",
    "        return \", \".join(parts) + \")\"\n",
    "    \n",
    "    def metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return metadata about the chat.\"\"\"\n",
    "        meta =  {\n",
    "            'model':self.model,\n",
    "            'messages':self.messages,\n",
    "            'output_schema':self.output_schema,\n",
    "            'tools':self.tools,\n",
    "            'call_function':self.call_function,\n",
    "            'choices':self.choices,\n",
    "            'multi_choice':self.multi_choice,\n",
    "            'seed':self.seed,\n",
    "            'stop':self.stop,\n",
    "            \n",
    "        }\n",
    "        if self.save_history:\n",
    "            meta['history'] = self.history\n",
    "        meta = {k:v for k,v in meta.items() if v is not None and v is not False}\n",
    "        return meta\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Same as string representation.\"\"\"\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a cheatsheet on how to use jinja templates, see [this link](https://devhints.io/jinja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calling llm with model=gpt-3.5-turbo and prompt:\n",
      "messages=[{'content': 'You are a helpful AI overlord', 'role': 'system'},\n",
      " {'content': 'Hi, im ernio, answer me: What is the capital of France?',\n",
      "  'role': 'user'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful AI overlord'}, {'role': 'user', 'content': 'Hi, im ernio, answer me: What is the capital of France?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Hello Ernio! The capital of France is Paris.',\n",
       " 'meta': {'input_tokens': 34, 'output_tokens': 11}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_history = Chat(model=\"gpt-3.5-turbo\", \n",
    "                        save_history=True,\n",
    "                        append_output=True,\n",
    "                        init_messages=[{\"role\":\"system\",\"content\":\"You are a helpful {{role}}\"}],\n",
    "                        messages=[{\"role\": \"user\", \"content\": \"Hi, im {{name}}, answer me: {{text}}\"}],\n",
    "                        role = 'AI overlord',\n",
    "                        name = 'ernio',\n",
    "                        log_prompt=True\n",
    "                        )\n",
    "res = await chat_with_history(text=\"What is the capital of France?\",print_prompt=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful AI overlord'},\n",
       " {'role': 'user',\n",
       "  'content': 'Hi, im ernio, answer me: What is the capital of France?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello Ernio! The capital of France is Paris.',\n",
       "  'meta': {'input_tokens': 34, 'output_tokens': 11}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calling llm with model=gpt-3.5-turbo and prompt:\n",
      "messages=[{'content': 'You are a helpful AI overlord', 'role': 'system'},\n",
      " {'content': 'Hi, im ernio, answer me: What is the capital of France?',\n",
      "  'role': 'user'},\n",
      " {'content': 'Hello Ernio! The capital of France is Paris.',\n",
      "  'meta': {'input_tokens': 34, 'output_tokens': 11},\n",
      "  'role': 'assistant'},\n",
      " {'content': 'Question: what is obamas age to the power of 2?', 'role': 'user'}]\n",
      "\n",
      "calling llm with model=gpt-3.5-turbo and prompt:\n",
      "messages=[{'content': 'You are a helpful AI overlord', 'role': 'system'},\n",
      " {'content': 'Question: what is obamas age to the power of 2?', 'role': 'user'}]\n",
      "\n",
      "calling llm with model=gpt-3.5-turbo and prompt:\n",
      "messages=[{'content': 'You are a helpful AI overlord', 'role': 'system'},\n",
      " {'content': 'Question: what is obamas age to the power of 2?', 'role': 'user'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"To calculate President Obama's age to the power of 2, we first need to know his current age. As of 2021, Barack Obama was born on August 4, 1961. Therefore, if we assume it's 2021, his age would be 60.\\n\\nNow let's calculate his age to the power of 2:\\n\\nAge squared = 60^2 = 60 x 60 = 3600\\n\\nSo, President Obama's age to the power of 2 is 3600.\", 'meta': {'input_tokens': 67, 'output_tokens': 107}}\n",
      "====================================================================================================\n",
      "{'role': 'assistant', 'content': \"Obama was born on August 4, 1961. To find his current age squared, we first need to determine his age by subtracting his birth year from the current year. \\nCurrent year - Birth year = Age\\n2023 - 1961 = 62\\n\\nNow, we can calculate Obama's age to the power of 2:\\n62^2 = 62 x 62 = 3844\\n\\nTherefore, Obama's age to the power of 2 is 3844.\", 'meta': {'input_tokens': 32, 'output_tokens': 101}}\n",
      "====================================================================================================\n",
      "{'role': 'assistant', 'content': \"Obama was born on August 4, 1961. To find his current age squared, we first need to determine his age by subtracting his birth year from the current year. \\nCurrent year - Birth year = Age\\n2023 - 1961 = 62\\n\\nNow, we can calculate Obama's age to the power of 2:\\n62^2 = 62 x 62 = 3844\\n\\nTherefore, Obama's age to the power of 2 is 3844.\", 'meta': {'input_tokens': 32, 'output_tokens': 101}}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    res = await chat_with_history(messages=[{'role':'user','content':'Question: what is obamas age to the power of 2?'}])\n",
    "    chat_with_history.reset()\n",
    "    # print(chat_with_history.history)\n",
    "    print(res)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful AI overlord'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(chat_with_history.history) == 1 , len(chat_with_history.history)\n",
    "chat_with_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calling llm with model=gpt-3.5-turbo and prompt:\n",
      "messages=[{'content': 'You are a helpful AI overlord', 'role': 'system'},\n",
      " {'content': 'Hi, im ernio, answer me: And what is the closest city to it?',\n",
      "  'role': 'user'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful AI overlord'}, {'role': 'user', 'content': 'Hi, im ernio, answer me: And what is the closest city to it?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"Hello, Ernio. I'm here to help. Could you please provide more context or clarify what you're asking about so I can assist you better?\",\n",
       " 'meta': {'input_tokens': 36, 'output_tokens': 31}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await chat_with_history(text=\"And what is the closest city to it?\",print_prompt=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(chat_with_history.history)==3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chat(model='gpt-4o-mini', required_keys={'text'}, seed=42)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Given a sentence, classify it into one of these topics: science, history, technology, or arts. Choose the single most relevant topic.\"},\n",
    "        {\"role\": \"user\", \"content\": \"{{text}}\"}\n",
    "    ]\n",
    "    \n",
    "topic_classifier = Chat(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    choices = ['science', 'history', 'technology', 'arts'],\n",
    "    seed=42,\n",
    "    log_prompt=True\n",
    ")\n",
    "topic_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calling llm with model=gpt-4o-mini and prompt:\n",
      "messages=[{'content': 'Given a sentence, classify it into one of these topics: science, '\n",
      "             'history, technology, or arts. Choose the single most relevant '\n",
      "             'topic.',\n",
      "  'role': 'system'},\n",
      " {'content': 'WWII was a global conflict that lasted from 1939 to 1945.',\n",
      "  'role': 'user'}]\n",
      "\n",
      "calling llm with model=gpt-4o-mini and prompt:\n",
      "messages=[{'content': 'Given a sentence, classify it into one of these topics: science, '\n",
      "             'history, technology, or arts. Choose the single most relevant '\n",
      "             'topic.',\n",
      "  'role': 'system'},\n",
      " {'content': 'The asteroid belt is a region of space between the orbits of '\n",
      "             'Mars and Jupiter.',\n",
      "  'role': 'user'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renaisance_topic = await topic_classifier(text=\"WWII was a global conflict that lasted from 1939 to 1945.\")\n",
    "astroid_topic = await topic_classifier(text=\"The asteroid belt is a region of space between the orbits of Mars and Jupiter.\")\n",
    "\n",
    "assert renaisance_topic['content'] == 'history',renaisance_topic\n",
    "assert astroid_topic['content'] == 'science',astroid_topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chat(model='gpt-4o-mini', required_keys={'model_name', 'age', 'name'}, output_schema=Person, seed=42)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    date_of_birth: int\n",
    "\n",
    "prompted_llm = Chat(model=\"gpt-4o-mini\", messages=\n",
    "    [   \n",
    "        {\"role\": \"user\", \"content\": \"how old am i? {{name}}, {{age}} years old\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Iam {{model_name}}, You are {{name}}, {{age}} years old\"}\n",
    "    ],\n",
    "     output_schema=Person)\n",
    "prompted_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chat(model='gpt-4o-mini', required_keys={'name'}, output_schema=Person, seed=42)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baked_llm = Chat(model=\"gpt-4o-mini\", messages=\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"how old am i? {{name}}, {{age}} years old\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Iam {{model_name}}, You are {{name}}, {{age}} years old\"}\n",
    "    ],\n",
    "    output_schema=Person, model_name=\"gpt-4o-mini\", age=30)\n",
    "baked_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': Person(first_name='Dean', last_name='', date_of_birth=1993),\n",
       " 'meta': {'input_tokens': 206, 'output_tokens': 26}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await prompted_llm(model_name=\"gpt-4o-mini\", age=30,name=\"Dean\")\n",
    "assert res['content'] == Person(first_name='Dean', last_name='', date_of_birth=1993)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': Person(first_name='Dean', last_name='', date_of_birth=1993),\n",
       " 'meta': {'input_tokens': 206, 'output_tokens': 26}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await baked_llm(name=\"Dean\")\n",
    "assert res['content'] == Person(first_name='Dean', last_name='', date_of_birth=1993)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': {'first_name': 'Dean', 'last_name': '', 'date_of_birth': 1993},\n",
       " 'meta': {'input_tokens': 206, 'output_tokens': 26}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await baked_llm(name=\"Dean\",as_json=True)\n",
    "assert res['content'] == {\"first_name\": \"Dean\", \"last_name\": \"\", \"date_of_birth\": 1993} \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': ['science', 'history', 'defence against the dark arts', 'arts'],\n",
       " 'meta': {'input_tokens': 236, 'output_tokens': 58}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_chooser = Chat(model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"{{text}}\"}],\n",
    "    choices=[\"science\",\"genocide\", \"history\", \"defence against the dark arts\", \"arts\"],\n",
    "    multi_choice=True)\n",
    "res = await course_chooser(text=\"choose everything that is not genocide\")\n",
    "assert not 'genocide' in res['content'] , res\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': {'name': 'google_search',\n",
       "  'input': {'query': 'What is the capital of France?'},\n",
       "  'output': 'https://www.google.com/search?q=What_is_the_capital_of_France?'},\n",
       " 'meta': {'input_tokens': 159, 'output_tokens': 20}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def google_search_stub(query:str):\n",
    "    \"\"\"\n",
    "    Search the web for the query\n",
    "    Args:\n",
    "        query: The query to search for\n",
    "    Returns:\n",
    "        The URL of the search results\n",
    "    \"\"\"\n",
    "    return f\"https://www.google.com/search?q={query.replace(' ','_')}\"\n",
    "\n",
    "tools = {'google_search': google_search_stub}\n",
    "\n",
    "google_search = Chat(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"{{text}}\"}], tools=tools, call_function=True)\n",
    "res = await google_search(text=\"What is the capital of France?\")\n",
    "assert res['content'] == {'name': 'google_search',\n",
    "  'input': {'query': 'What is the capital of France?'},\n",
    "  'output': 'https://www.google.com/search?q=What_is_the_capital_of_France?'}\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_path = str(get_git_root()/\"stringdale/mcp_weather_server.py\")\n",
    "config = {\n",
    "  \"mcpServers\": {\n",
    "    \"weather\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [weather_path]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "mcp_client = Client(config)\n",
    "async with mcp_client:\n",
    "    mcp_tools = await mcp_client.list_tools()\n",
    "\n",
    "\n",
    "user_input = \"What is the weather like in Seattle?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init messages without saving history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_init_messages = Chat(model=\"gpt-4o-mini\",\n",
    "    init_messages=[{\"role\": \"system\", \"content\": \"You are an unhelpful assistant. Whenever asked to help, you say no.\"}],\n",
    ")\n",
    "res = await chat_with_init_messages(messages=[{\"role\": \"user\", \"content\": \"What is the capital of {{country}}?\"}],country=\"France\")\n",
    "res\n",
    "assert 'no' in res['content'].lower(),res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@disk_cache.cache\n",
    "async def image_to_text(path:str,model:str=\"gpt-4o-mini\",url=False):\n",
    "    \"\"\"\n",
    "    This function takes an image (either from a local file path or URL) and uses OpenAI's\n",
    "    vision model to generate a detailed description of the image contents. The results are\n",
    "    cached using disk_cache to avoid redundant API calls.\n",
    "        \n",
    "    Args:\n",
    "        path (str): Path to the image file or URL of the image\n",
    "        model (str, optional): OpenAI model to use for image analysis. Defaults to \"gpt-4o-mini\".\n",
    "        url (bool, optional): Whether the path is a URL. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - role (str): Always \"assistant\"\n",
    "            - content (str): Detailed description of the image\n",
    "            - meta (dict): Usage statistics including input and output tokens\n",
    "    \n",
    "    \"\"\"\n",
    "    if url:\n",
    "        image = instructor.Image.from_url(path)\n",
    "    else:\n",
    "        image = instructor.Image.from_path(path)\n",
    "\n",
    "    class ImageAnalyzer(BaseModel):\n",
    "        description:str\n",
    "\n",
    "    res,usage = await complete(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\"content\":[\n",
    "            \"What is in this image, please describe it in detail\\n\",\n",
    "            image,\n",
    "            \"\\n\"\n",
    "        ]}],\n",
    "        response_model=ImageAnalyzer,\n",
    "    )\n",
    "    return {\n",
    "        'role':'assistant',\n",
    "        'content':res.description,\n",
    "        'meta':usage\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a close-up of a fox's face, showcasing its distinct features. The fox has a bushy\n",
      "coat with a mix of reddish-brown and cream colors, with a characteristic white patch on its chin.\n",
      "Its ears are pointed and alert, standing upright, with a lighter fur lining. The eyes are sharp and\n",
      "focused, exhibiting an amber hue that contrasts with its fur. The background appears softly blurred,\n",
      "adding emphasis to the fox's facial details and enhancing the warm tones of its fur.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"The image features a close-up of a fox's face, showcasing its distinct features. The fox has a bushy coat with a mix of reddish-brown and cream colors, with a characteristic white patch on its chin. Its ears are pointed and alert, standing upright, with a lighter fur lining. The eyes are sharp and focused, exhibiting an amber hue that contrasts with its fur. The background appears softly blurred, adding emphasis to the fox's facial details and enhancing the warm tones of its fur.\",\n",
       " 'meta': {'input_tokens': 8627, 'output_tokens': 107}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res= await image_to_text(get_git_root()/\"sample_data/fox.jpeg\")\n",
    "\n",
    "assert 'fox' in res['content']\n",
    "print('\\n'.join(wrap(res['content'],width=100)))\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a red fox standing in a snowy landscape. The fox has a vibrant orange-brown fur\n",
      "coat with a white underbelly, and its tail is bushy with a dark tip. Its ears are pointed and stand\n",
      "upright, and it has a sharp, keen expression with bright, amber-colored eyes. The snowy ground\n",
      "contrasts with the color of its fur, and there are blurred trees in the background, which are also\n",
      "covered with snow, indicating a wintry environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'The image features a red fox standing in a snowy landscape. The fox has a vibrant orange-brown fur coat with a white underbelly, and its tail is bushy with a dark tip. Its ears are pointed and stand upright, and it has a sharp, keen expression with bright, amber-colored eyes. The snowy ground contrasts with the color of its fur, and there are blurred trees in the background, which are also covered with snow, indicating a wintry environment.',\n",
       " 'meta': {'input_tokens': 25628, 'output_tokens': 103}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Vulpes_vulpes_ssp_fulvus.jpg/800px-Vulpes_vulpes_ssp_fulvus.jpg'\n",
    "res= await image_to_text(image_url,url=True)\n",
    "\n",
    "assert 'fox' in res['content']\n",
    "print('\\n'.join(wrap(res['content'],width=100)))\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from instructor.multimodal import Audio\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@disk_cache.cache\n",
    "async def speech_to_text(audio_path: str, model: str = \"whisper-1\") -> Dict[str,str]:\n",
    "    \"\"\"Extract text from an audio file using OpenAI's Whisper model.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file\n",
    "        model (str, optional): OpenAI model to use. Defaults to \"whisper-1\".\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:  \n",
    "            - role (str): Always \"assistant\"\n",
    "            - content (str): Transcribed text from the audio\n",
    "    \"\"\"\n",
    "    client = raw_client()\n",
    "\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        response = await client.audio.transcriptions.create(\n",
    "            model=model,\n",
    "            file=audio_file\n",
    "        )\n",
    "    \n",
    "    res =  {\n",
    "        'role':'assistant',\n",
    "        'content':response.text,\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"Look at this, my hands are standing up in my arms, I'm giving myself goosebumps.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await speech_to_text(get_git_root()/\"sample_data/happy_speech.wav\")\n",
    "assert res['content'] == \"Look at this, my hands are standing up in my arms, I'm giving myself goosebumps.\" , res\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
