# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/024_llms.ipynb.

# %% auto 0
__all__ = ['logger', 'json_openai_client', 'raw_openai_client', 'mcp_tools_to_openai_kwargs', 'complete_open_ai',
           'mcp_tools_to_anthropic_kwargs', 'complete_anthropic', 'complete_raw', 'complete', 'answer_question',
           'choose', 'choose_many', 'clean_model', 'structured_output', 'User', 'Chat', 'image_to_text',
           'speech_to_text']

# %% ../nbs/024_llms.ipynb 4
from .core import get_git_root, load_env, checkLogs,  json_render,json_undeclared_vars,disk_cache
import openai 
from pydantic import BaseModel, create_model
from typing import Optional, Dict, Any, List, Union
import json
import re
from parse import parse
from pathlib import Path
from enum import Enum
from pydantic import BaseModel
import logging
import os
from .core import semaphore_decorator


# %% ../nbs/024_llms.ipynb 5
load_env()
logger = logging.getLogger(__name__)


# %% ../nbs/024_llms.ipynb 6
from typing import (
    Callable, 
    Type, 
    Optional, 
    List, 
    Dict, 
    Any, 
    Union,
    Literal,
    get_type_hints
)
from pydantic import (
    BaseModel, 
    Field, 
    create_model,
    ConfigDict
)
import inspect
import sys
import sqlmodel
from typing import Optional
# TODO make chat outputschema also get typing (like bool)

# %% ../nbs/024_llms.ipynb 9
import instructor
from openai import OpenAI,AsyncOpenAI
from anthropic import Anthropic, AsyncAnthropic
from pydantic import BaseModel
from singleton_decorator import singleton
import asyncio


# %% ../nbs/024_llms.ipynb 12
## OpenAI clients
@singleton
def json_openai_client():
    return instructor.from_openai(AsyncOpenAI(),mode=instructor.Mode.JSON)
@singleton
def raw_openai_client():
    return AsyncOpenAI()

# %% ../nbs/024_llms.ipynb 13
def mcp_tools_to_openai_kwargs(mcp_tools):
    """
    Convert MCP tools to OpenAI's tools parameter format.
    
    Args:
        mcp_tools: List of MCP Tool objects (can be empty)
        
    Returns:
        Dict with 'tools' key containing OpenAI-formatted tools, or empty dict if no tools
    """
    if not mcp_tools:
        return {}
    
    openai_tools = [{
        "type": "function",
        "function": {
            "name": tool.name,
            "description": tool.description,
            "parameters": tool.inputSchema
        }
    } for tool in mcp_tools]
    
    return {"tools": openai_tools}


# %% ../nbs/024_llms.ipynb 14
@disk_cache.cache(ignore=['response_model'])
async def complete_open_ai(model, messages, response_model=None, response_schema=None, mode='json', seed=42, **kwargs):
    """
    OpenAI-specific completion handler.
    Chooses between clients based on mode and runs the completion.
    """
    if mode == 'json':
        client = json_openai_client()
        response, completion = await client.chat.completions.create_with_completion(
            model=model,
            messages=messages,
            response_model=response_model,
            seed=seed,
            **kwargs
        )
        usage = {
            "input_tokens": completion.usage.prompt_tokens,
            "output_tokens": completion.usage.completion_tokens
        }
        return response.model_dump_json(), usage


    elif mode == 'mcp_tools' or mode == 'raw':
        # Unified path: handle both raw (no tools) and mcp_tools cases
        client = raw_openai_client()
        
        # Get MCP tools from kwargs (default to empty list for 'raw' mode)
        mcp_tools = kwargs.pop('mcp_tools', [])
        
        # Convert MCP tools to OpenAI format
        tools_kwargs = mcp_tools_to_openai_kwargs(mcp_tools)
        
        # Filter out unsupported parameters if needed
        openai_kwargs = {k: v for k, v in kwargs.items() 
                        if k not in ['print_prompt']}
        
        # Call OpenAI API
        completion = await client.chat.completions.create(
            model=model,
            messages=messages,
            seed=seed,
            **tools_kwargs,  # Includes tools if any
            **openai_kwargs
        )
        
        # Extract usage information
        usage = {
            "input_tokens": completion.usage.prompt_tokens,
            "output_tokens": completion.usage.completion_tokens
        }
        
        # Parse response into unified format
        message = completion.choices[0].message
        result = {
            "text": message.content if message.content else None
        }
        
        # Extract tool calls if any
        if message.tool_calls:
            result["tool_calls"] = []
            for tool_call in message.tool_calls:
                result["tool_calls"].append({
                    "name": tool_call.function.name,
                    "input": json.loads(tool_call.function.arguments),
                    "id": tool_call.id
                })
        
        # For backward compatibility with 'raw' mode: if no tools and no tool_calls, return just text string
        if mode == 'raw' and not result.get("tool_calls") and not mcp_tools:
            return result["text"], usage
        
        return result, usage
    
    else:
        raise ValueError(f"Invalid mode: {mode}")

# %% ../nbs/024_llms.ipynb 20
def mcp_tools_to_anthropic_kwargs(mcp_tools):
    """
    Convert MCP tools to Anthropic's tools parameter format.
    
    Args:
        mcp_tools: List of MCP Tool objects (can be empty)
        
    Returns:
        Dict with 'tools' key containing Anthropic-formatted tools, or empty dict if no tools
    """
    if not mcp_tools:
        return {}
    
    anthropic_tools = [{
        "name": tool.name,
        "description": tool.description,
        "input_schema": tool.inputSchema
    } for tool in mcp_tools]
    
    return {"tools": anthropic_tools}

# %% ../nbs/024_llms.ipynb 21
@disk_cache.cache(ignore=['response_model'])
async def complete_anthropic(model, messages, response_model=None, response_schema=None, mode='json', seed=42, **kwargs):
    """
    Anthropic-specific completion handler.
    Chooses between clients based on mode and runs the completion.
    """
    def _extract_usage(usage_obj):
        """Extract usage information from Anthropic usage object"""
        return {
            "input_tokens": getattr(usage_obj, "input_tokens", 0),
            "output_tokens": getattr(usage_obj, "output_tokens", 0)
        }
    
    max_tokens = kwargs.get('max_tokens', 1024)
    # Filter out unsupported Anthropic parameters
    # Anthropic doesn't support 'stop' or 'seed' parameters
    anthropic_kwargs = {k: v for k, v in kwargs.items() 
                        if k not in ['stop', 'seed', 'print_prompt', 'mcp_tools']}
    
    if mode == 'json':
        client = json_anthropic_client()
        response, completion = await client.chat.completions.create_with_completion(
            model=model,
            messages=messages,
            response_model=response_model,
            max_tokens=max_tokens,
            **anthropic_kwargs
        )
        return response.model_dump_json(), _extract_usage(completion.usage)

    elif mode == 'mcp_tools' or mode == 'raw':
        # Unified path: handle both raw (no tools) and mcp_tools cases
        client = raw_anthropic_client()
        
        # Get MCP tools from kwargs (default to empty list for 'raw' mode)
        mcp_tools = kwargs.pop('mcp_tools', [])
        
        # Convert MCP tools to Anthropic format
        tools_kwargs = mcp_tools_to_anthropic_kwargs(mcp_tools)
        
        # Extract system message (first system message), rest as regular messages
        system = None
        stripped = []
        for m in messages:
            role = m.get("role")
            content = m.get("content", "")
            if role == "system" and system is None:
                system = content
            else:
                stripped.append({"role": role, "content": content})
        
        # Call Anthropic API
        response = await client.messages.create(
            model=model,
            system=system if system else "",  # Anthropic allows empty string for system
            messages=stripped,
            max_tokens=max_tokens,
            **tools_kwargs,  # Includes tools if any
            **anthropic_kwargs
        )
        
        # Parse response content blocks
        text_parts = []
        tool_use_blocks = []
        
        for block in (response.content or []):
            if getattr(block, "type", None) == "text":
                text_parts.append(block.text)
            elif getattr(block, "type", None) == "tool_use":
                # Extract tool use information
                tool_use_blocks.append({
                    "name": block.name,
                    "input": block.input,  # Already a dict, no need to parse JSON
                    "id": getattr(block, "id", None)  # Include id if available
                })
        
        # Extract usage information
        usage = _extract_usage(response.usage)
        
        # Return consistent structure: always a dict with 'text' and optionally 'tool_calls'
        result = {
            "text": "\n".join(text_parts) if text_parts else None
        }
        if tool_use_blocks:
            result["tool_calls"] = tool_use_blocks
        
        # For backward compatibility with 'raw' mode: if no tools and no tool_calls, return just text string
        if mode == 'raw' and not tool_use_blocks and not mcp_tools:
            return result["text"], usage
        
        return result, usage
    
    else:
        raise ValueError(f"Invalid mode: {mode}")

# %% ../nbs/024_llms.ipynb 28
async def complete_raw(model, messages, response_model=None, response_schema=None, mode='json', seed=42, provider=None, **kwargs):
    """
    This function is used to complete a chat completion with instructor without having basemodels as input or output.
    used for disk caching of results.
    
    Chooses what provider and calls a provider specific caller by model name with provider override.
    
    Args:
        model: Model name (e.g., 'gpt-4', 'claude-3-haiku-20240307')
        messages: List of message dicts
        response_model: Pydantic model for structured output
        response_schema: Schema for response (computed if response_model is provided)
        mode: Completion mode ('raw', 'json', 'tools')
        seed: Random seed
        provider: Optional provider override ('openai', 'anthropic'). If None, inferred from model name.
        **kwargs: Additional arguments passed to the provider.
                  Important provider-specific requirements:
                  - Anthropic: max_tokens is REQUIRED (e.g., max_tokens=1024)
    """
    # Determine provider from model name or use override
    if provider is None:
        if model.startswith(('gpt-', 'o1-')):
            provider = 'openai'
        elif model.startswith('claude-'):
            provider = 'anthropic'
        else:
            # Default to OpenAI for unknown models
            provider = 'openai'
    
    # Route to provider-specific function
    if provider == 'openai':
        return await complete_open_ai(
            model=model,
            messages=messages,
            response_model=response_model,
            response_schema=response_schema,
            mode=mode,
            seed=seed,
            **kwargs
        )
    elif provider == 'anthropic':
        return await complete_anthropic(
            model=model,
            messages=messages,
            response_model=response_model,
            response_schema=response_schema,
            mode=mode,
            **kwargs
        )
    else:
        raise ValueError(f"Unknown provider: {provider}")

async def complete(model, messages, response_model, mode='json', print_prompt=False, **kwargs):
    # Compute schema if response model provided
    if isinstance(response_model, type) and issubclass(response_model, BaseModel):
        response_schema = response_model.model_json_schema()
    else:
        response_schema = str(response_model)
    if print_prompt:
        print(messages)
    response, usage = await complete_raw(
        model=model,
        messages=messages,
        response_model=response_model,
        response_schema=response_schema,
        mode=mode,
        **kwargs
    )
    if mode == 'raw' or mode == 'mcp_tools':
        return response, usage
    else:
        # Check if response is already a model instance (from instructor)
        if isinstance(response, BaseModel):
            return response, usage
        else:
            return response_model.model_validate_json(response), usage

# %% ../nbs/024_llms.ipynb 33
async def answer_question(model,messages,**api_kwargs):
    res,usage = await complete(model,messages,response_model=None,mode='raw',**api_kwargs)
    return res,usage


# %% ../nbs/024_llms.ipynb 37
async def choose(model,messages,choices,**api_kwargs):
    class Choice(BaseModel):
        choice: Literal[tuple(choices)]
    res,usage = await complete(model,messages,Choice,**api_kwargs)
    return res.choice,usage


# %% ../nbs/024_llms.ipynb 41
async def choose_many(model,messages,choices,**api_kwargs):
    class Choice(BaseModel):
        choice: Literal[tuple(choices)]

    class Choices(BaseModel):
        choices: List[Choice]   
    res,usage = await complete(model,messages,Choices,**api_kwargs)
    return [c.choice for c in res.choices],usage


# %% ../nbs/024_llms.ipynb 45
def clean_model(model: Type[sqlmodel.SQLModel], name: Optional[str] = None) -> Type[BaseModel]:
    """Convert an SQLModel to a Pydantic BaseModel.
    used to clean up the output for the LLM
    Args:
        model: SQLModel class to convert
        name: Optional name for the new model class
        
    Returns:
        A Pydantic BaseModel class with the same fields
    """
    # Get field definitions from the SQLModel
    fields = {}
    for field_name, field in model.model_fields.items():
        fields[field_name] = (field.annotation, field)
    
    # Create new model name if not provided
    model_name = name or f"{model.__name__}Schema"
    
    # Create and return new Pydantic model
    return create_model(model_name, **fields)

# %% ../nbs/024_llms.ipynb 46
async def structured_output(model,messages,output_schema,as_json=False,**api_kwargs):

    is_sqlmodel = isinstance(output_schema,type) and issubclass(output_schema,sqlmodel.SQLModel)
    if is_sqlmodel:
        clean_schema = clean_model(output_schema)
    else:
        clean_schema = output_schema

    res,usage = await complete(model,messages,clean_schema,**api_kwargs)

    if is_sqlmodel:
        res = output_schema(**res.model_dump())
    if as_json:
        res = res.model_dump()
    return res,usage


# %% ../nbs/024_llms.ipynb 50
class User(sqlmodel.SQLModel, table=False):
    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)
    name: Optional[str] = sqlmodel.Field(default=None)
    age: Optional[int] = sqlmodel.Field(default=None)
    email: Optional[str] = sqlmodel.Field(default=None)



# %% ../nbs/024_llms.ipynb 54
from copy import deepcopy,copy
from pprint import pformat,pprint


# %% ../nbs/024_llms.ipynb 55
class Chat:
    """A Chat objects the renders a prompt and calls an LLM. Currently supporting openai models.
    
    Args:
        model: OpenAI model name
        messages: List of message dicts, must have at least a role and content field
        output_schema: Optional schema for structured output
        as_json: Optional boolean to return the response as a json object
        tools: Optional dictionary of tool names and functions that the LLM can decide to call. Causes the content of the response
            to be a dict of the form {'name':tool_name,'input':tool_input_dict}
        call_function: if tools are provided, whether to call the function and save the output in the output field of the response's content
        choices: Optional List of choices for multi-choice questions
        multi_choice: if choices are provided, whether to choose multiple items from the list
        seed: Optional seed for random number generation
        stop: Optional string or list of strings where the model should stop generating
        save_history: Optional boolean to save the history of the chat between calls
        append_output: Optional, whether to append the output of the chat to history automatically, default False
        init_messages: Optional list of messages that are always prepended to messages.
            Useful for supplying additional messages during calls.
            Can have template variables that are fed during initialization only.
            If save_history is True, the init messages are added to the history.
        **kwargs: Keyword arguments to interpolate into the messages
    """
    def __init__(self,
        model: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None, 
        output_schema: Optional[BaseModel] = None,
        as_json: Optional[bool] = False,
        tools: Optional[Dict[str,Callable]] = None,
        mcp_tools: Optional[Dict[str,Callable]] = None,
        call_function: Optional[bool] = False,
        choices: Optional[Enum] = None,
        multi_choice: Optional[bool] = False,
        seed: Optional[int] = 42,
        stop: Optional[Union[str, List[str]]] = None,
        log_prompt: bool = False,
        save_history: bool = False,
        append_output: bool = False,
        init_messages: Optional[List[Dict[str, str]]] = None,
        **kwargs):

        self.model = model
        self.messages = deepcopy(messages)
        self.output_schema = output_schema
        self.as_json = as_json
        self.mcp_tools = mcp_tools
        self.call_function = call_function
        self.choices = choices
        self.multi_choice = multi_choice
        self.seed = seed
        self.stop = stop
        self.log_prompt = log_prompt
        self.baked_kwargs = kwargs
        self.save_history = save_history
        self.append_output = append_output
        
        if init_messages is None:
            init_messages = []
        self.init_messages = json_render(init_messages,context=kwargs)
    
        self.reset()

    def reset(self):
        """Resets state of Chat"""
        if self.save_history:
            self.history = []
            self.history.extend(self.init_messages)
        else:
            self.history = None
    
    def dump_state(self):
        """dumps the node state"""
        return self.history

    def load_state(self,state_object):
        """loads node state"""
        self.history = state_object

    def __copy__(self):
        chat_copy = Chat(
            model=self.model,
            messages=self.messages,
            output_schema=self.output_schema,
            as_json=self.as_json,
            tools=self.tools,
            mcp_tools=self.mcp_tools,
            call_function=self.call_function,
            choices=self.choices,
            multi_choice=self.multi_choice,
            seed=self.seed,
            stop=self.stop,
            log_prompt=self.log_prompt,
            save_history=self.save_history,
            append_output=self.append_output,
            init_messages=self.init_messages,
            **self.baked_kwargs
        )
        return chat_copy

    async def __call__(self, **kwargs) -> Dict[str, Any]:
        """Format prompt with kwargs and call OpenAI chat.
        Init parameters such as output_schema, tools, choices, seed, stop, as well as template variables
        can be set or overridden by kwargs
        
        Args:
            **kwargs: Values for format string placeholders
            
        Returns:
            a dictionary with the following keys:
            - role (str): Always "assistant"
            - content: the llm response.
            - meta (dict): Usage statistics including input and output tokens
        """
        model = kwargs.get("model", self.model)
        messages = kwargs.get("messages", self.messages)
        output_schema = kwargs.get("output_schema", self.output_schema)
        as_json = kwargs.get("as_json", self.as_json)
        mcp_tools = kwargs.get("mcp_tools", self.mcp_tools)
        call_function = kwargs.get("call_function", self.call_function)
        choices = kwargs.get("choices", self.choices)
        multi_choice = kwargs.get("multi_choice", self.multi_choice)
        seed = kwargs.get("seed", self.seed)
        stop = kwargs.get("stop", self.stop)

        if model is None:
            raise ValueError("model is required but not provided")
        if messages is None:
            raise ValueError("messages is required but not provided")

        prompt_kwargs = {**self.baked_kwargs, **kwargs}

        required_kwargs = json_undeclared_vars(messages)
        if not required_kwargs <= set(prompt_kwargs):
            missing = required_kwargs - set(prompt_kwargs)
            raise ValueError(f"Missing required kwargs: {missing}")

        formatted_messages = json_render(messages, context=prompt_kwargs)

        if self.save_history:
            self.history.extend(formatted_messages)
            formatted_messages = self.history
        else:
            formatted_messages = self.init_messages + formatted_messages

        if self.log_prompt:
            logger.warning(f'calling llm with model={model} and prompt:\n'
                        f'messages={pformat(formatted_messages)}\n'
                        )

        completion_kwargs = {
            'model':model,
            'messages':formatted_messages,
            'seed':seed,
            'stop':stop,
            'print_prompt':prompt_kwargs.get('print_prompt',False),
        }

        if choices:
            if multi_choice:
                res,usage = await choose_many(choices=choices,**completion_kwargs)
            else:
                res,usage = await choose(choices=choices,**completion_kwargs)
        elif output_schema:
            res,usage = await structured_output(output_schema=output_schema,as_json=as_json,**completion_kwargs)
        elif mcp_tools:
            res, usage = await complete(
                mode='mcp_tools',
                mcp_tools=mcp_tools,
                response_model=None,
                **completion_kwargs
            )
        else:
            res,usage = await answer_question(**completion_kwargs)

        response = {
            'role':'assistant',
            'content':res,
            'meta':usage
        }
        if self.save_history and self.append_output:
            self.history.append(response)
        return response
    
    def __str__(self) -> str:
        """String representation showing required keys, model, and output schema."""
        parts = [f"Chat(model='{self.model}'"]
        
        if self.messages:
            required_keys = json_undeclared_vars(self.messages) - set(self.baked_kwargs.keys())
            parts.append(f"required_keys={required_keys}")
            
        if self.output_schema:
            parts.append(f"output_schema={self.output_schema.__name__}")
        
        if self.tools:
            parts.append(f"""tools={",".join(self.tools.keys())}""")
        if self.call_function:
            parts.append(f"call_function={self.call_function}")
            
        # if self.choices:
        #     parts.append(f"choices={self.choices}")
        # if self.multi_choice:
        #     parts.append(f"multi_choice={self.multi_choice}")
            
        if self.seed:
            parts.append(f"seed={self.seed}")
            
        if self.stop:
            parts.append(f"stop={self.stop}")

        if self.save_history:
            parts.append(f"save_history={self.save_history}")
            
        return ", ".join(parts) + ")"
    
    def metadata(self) -> Dict[str, Any]:
        """Return metadata about the chat."""
        meta =  {
            'model':self.model,
            'messages':self.messages,
            'output_schema':self.output_schema,
            'tools':self.tools,
            'call_function':self.call_function,
            'choices':self.choices,
            'multi_choice':self.multi_choice,
            'seed':self.seed,
            'stop':self.stop,
            
        }
        if self.save_history:
            meta['history'] = self.history
        meta = {k:v for k,v in meta.items() if v is not None and v is not False}
        return meta

    def __repr__(self) -> str:
        """Same as string representation."""
        return self.__str__()

# %% ../nbs/024_llms.ipynb 80
@disk_cache.cache
async def image_to_text(path:str,model:str="gpt-4o-mini",url=False):
    """
    This function takes an image (either from a local file path or URL) and uses OpenAI's
    vision model to generate a detailed description of the image contents. The results are
    cached using disk_cache to avoid redundant API calls.
        
    Args:
        path (str): Path to the image file or URL of the image
        model (str, optional): OpenAI model to use for image analysis. Defaults to "gpt-4o-mini".
        url (bool, optional): Whether the path is a URL. Defaults to False.
        
    Returns:
        dict: A dictionary containing:
            - role (str): Always "assistant"
            - content (str): Detailed description of the image
            - meta (dict): Usage statistics including input and output tokens
    
    """
    if url:
        image = instructor.Image.from_url(path)
    else:
        image = instructor.Image.from_path(path)

    class ImageAnalyzer(BaseModel):
        description:str

    res,usage = await complete(
        model=model,
        messages=[{"role":"user","content":[
            "What is in this image, please describe it in detail\n",
            image,
            "\n"
        ]}],
        response_model=ImageAnalyzer,
    )
    return {
        'role':'assistant',
        'content':res.description,
        'meta':usage
    }


# %% ../nbs/024_llms.ipynb 86
from instructor.multimodal import Audio
import openai

# %% ../nbs/024_llms.ipynb 87
@disk_cache.cache
async def speech_to_text(audio_path: str, model: str = "whisper-1") -> Dict[str,str]:
    """Extract text from an audio file using OpenAI's Whisper model.
    
    Args:
        audio_path (str): Path to the audio file
        model (str, optional): OpenAI model to use. Defaults to "whisper-1".
    
    Returns:
        dict: A dictionary containing:  
            - role (str): Always "assistant"
            - content (str): Transcribed text from the audio
    """
    client = raw_client()

    with open(audio_path, "rb") as audio_file:
        response = await client.audio.transcriptions.create(
            model=model,
            file=audio_file
        )
    
    res =  {
        'role':'assistant',
        'content':response.text,
    }
    
    return res
